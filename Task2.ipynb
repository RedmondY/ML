{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习内容\n",
    "#### 1. 逻辑回归与线性回归的联系与区别\n",
    "  \n",
    "logistic回归与线性回归实际上有很多相同之处，最大的区别就在于他们的因变量不同，其他的基本都差不多，正是因为如此，这两种回归可以归于同一个家族，即广义线性模型（generalized linear model）。这一家族中的模型形式基本上都差不多，不同的就是因变量不同，如果是连续的，就是多重线性回归，如果是二项分布，就是logistic回归。logistic回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以实际中最为常用的就是二分类的logistic回归。  \n",
    "区别在于逻辑回归多了一个Sigmoid函数，使样本能映射到[0,1]之间的数值，用来做分类问题。在线性回归模型中，输出一般是连续的， 对于每一个输入的x，都有一个对应的输出y。因此模型的定义域和值域都可以是无穷。但是对于逻辑回归，输入可以是连续的[-∞, +∞]，但输出一般是离散的，通常只有两个值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 逻辑回归的原理\n",
    "\n",
    "[链接](https://blog.csdn.net/qq_41577045/article/details/80302991)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 逻辑回归损失函数推导及优化\n",
    "\n",
    "[链接](https://blog.csdn.net/gangyin5071/article/details/81280019)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 正则化与模型评估指标\n",
    "\n",
    "##### 正则化\n",
    "正则化就是在损失函数后加上一个正则化项（惩罚项），其实就是常说的结构风险最小化策略，即经验风险（损失函数）加上正则化。一般模型越复杂，正则化值越大。 正则化是用来对模型中某些参数进行约束，目的是防止过拟合。 \n",
    "\n",
    "##### 评估指标\n",
    "分类模型： 准确率（Accuracy）、精确率（Precision）、召回率（Recall）、F1-sore(精确率和召回率的调和平均) 、AUC（ROC曲线下面积） 回归模型： 平方根误差（RMSE）、误差分位数（Quantiles of Errors）、Almost Crrect” Predictions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 逻辑回归的优缺点\n",
    "\n",
    "#####  优点：\n",
    "\n",
    "形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。  \n",
    "模型效果不错。在工程上是可以接受的（作为 baseline），如果特征工程做的好，效果不会太差，并且特征工程可以并行开发，大大加快开发的速度。  \n",
    "训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化 SGD 发展比较成熟。  \n",
    "方便调整输出结果，通过调整阈值的方式。  \n",
    "  \n",
    "#####  缺点：\n",
    "  \n",
    "准确率欠佳。因为形式非常的简单，而现实中的数据非常复杂，因此，很难达到很高的准确性。  \n",
    "很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1。我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。  \n",
    "无法自动的进行特征筛选。  \n",
    "只能处理二分类问题。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 样本不均衡问题解决办法\n",
    "\n",
    "#####  样本的过采样和欠采样  \n",
    "过采样： 将稀有类别的样本进行复制，通过增加此稀有类样本的数量来平衡数据集.该方法适用于数据量较小的情况.  \n",
    "欠采样： 从丰富类别的样本中随机选取和稀有类别相同数目的样本，通过减少丰富类的样本量啦平衡数据集.该方法适用于数据量较大的情况.  \n",
    "SMOTE构造采样: 基于距离度量的方式计算两个或多个稀有类样本之间的相似性.然后选择其中的一个样本作为基础样本，再在邻居样本中随机选取一定数量的样本对那个基础样本的一个属性进行噪声.每次处理一个属性，通过这样的方式产生新生数据.  \n",
    "#####  对原数据的权值进行改变\n",
    "通过改变多数类样本和少数类样本数据在训练时的权重来解决样本不均衡的问题，是指在训练分类器时，为少数类样本赋予更大的权值，为多数类样本赋予较小的权值.  \n",
    "#####  通过组合集成方法解决\n",
    "将多数类数据随机分成少数类数据的量N份，每一份与全部的少数类数据一起训练成为一个分类器，这样反复训练会生成很多的分类器。最后再用组合的方式(bagging或者boosting)对分类器进行组合，得到更好的预测效果。简单来说若是分类问题可采用投票法，预测问题可以采用平均值。这个解决方式需要很强的计算能力以及时间，但效果较好，相当于结合了组合分类器的优势。  \n",
    "#####  通过特征选择\n",
    "在样本数据较为不均衡，某一类别数据较少的情况下，通常会出现特征分布很不均衡的情况。例如文本分类中，有大量的特征可以选择。因此我们可以选择具有显著区分能力的特征进行训练，也能在一定程度上提高模型的泛化效果。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. sklearn参数\n",
    "\n",
    "[链接](https://www.cnblogs.com/tosouth/p/4887917.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
